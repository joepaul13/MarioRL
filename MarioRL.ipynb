{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiS2Md/6iXsA6tH+Yjj76H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joepaul13/MarioRL/blob/main/MarioRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCYNu1A8LArk",
        "outputId": "4e299936-7461-4739-f791-2a145aac265e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-super-mario-bros==7.4.0\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 KB 11.7 MB/s eta 0:00:00\n",
            "Collecting nes-py>=8.1.4\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 KB 10.4 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.9/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.22.4)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 58.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.9/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.65.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.15.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py): started\n",
            "  Building wheel for nes-py (setup.py): finished with status 'done'\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp39-cp39-linux_x86_64.whl size=494913 sha256=34db8d8881c624e7e8ae58eaf1c0e0dd1f2b088d2b9991fbf4982f9ac4c55382\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/e1/4b/dbbd5d4a46ad80c0149d5671edb272c728c130e4d5750ca1d2\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install gym-super-mario-bros==7.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, time, datetime, os, copy"
      ],
      "metadata": {
        "id": "3ng5P48dLadJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros"
      ],
      "metadata": {
        "id": "muVpfb5iNluJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if gym.__version__ < '0.26':\n",
        "  env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api = True)\n",
        "else:\n",
        "  env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode = 'rgb', apply_api_compatibility = True)\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n, {info}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBngsSqTOBzN",
        "outputId": "aea4a312-cf00-47f6-e911-f53e652b7208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3),\n",
            " 0.0,\n",
            " False,\n",
            ", {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "  def __init__(self, env, skip):\n",
        "    super().__init__(env)\n",
        "    self._skip = skip\n",
        "\n",
        "  def step(self, action):\n",
        "    total_reward = 0.0\n",
        "    for i in range(self._skip):\n",
        "      obs, reward, done, trunk, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    return obs, total_reward, done, trunk, info"
      ],
      "metadata": {
        "id": "V-yum_clQdu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "  def __init__(self, env):\n",
        "    super().__init__(env)\n",
        "    obs_shape = self.observation_space.shape[:2]\n",
        "    self.observation_space = Box(low = 0, high = 255, shape = obs_shape, dtype = np.uint8)\n",
        "\n",
        "  def permute_orientation(self, observation):\n",
        "    observation = np.transpose(observation, (2, 0, 1))\n",
        "    observation = torch.tensor(observation.copy(), dtype = torch.float)\n",
        "    return observation\n",
        "  \n",
        "  def observation(self, observation):\n",
        "    observation = self.permute_orientation(observation)\n",
        "    transform = T.Grayscale()\n",
        "    observation = transform(observation)\n",
        "    return observation"
      ],
      "metadata": {
        "id": "_CvUoQp7Sq0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "  def __init__(self, env, shape):\n",
        "    super().__init__(env)\n",
        "    if isinstance(shape, int):\n",
        "      self.shape = (shape, shape)\n",
        "    else:\n",
        "      self.shape = tuple(shape)\n",
        "\n",
        "    obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "    self.observation_space = Box(low = 0, high = 255, shape = obs_shape, dtype = np.uint8)\n",
        "\n",
        "  def observation(self, observation):\n",
        "    transforms = T.Compose([T.Resize(self.shape), T.Normalize(0, 255)])\n",
        "    observation = transforms(observation).squeeze(0)\n",
        "    return observation"
      ],
      "metadata": {
        "id": "cwnF0-IoUZp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SkipFrame(env, skip = 4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape = 84)\n",
        "if gym.__version__ < '0.26':\n",
        "  env = FrameStack(env, num_stack = 4, new_step_api = True)\n",
        "else:\n",
        "  env = FrameStack(env, num_stack = 4)"
      ],
      "metadata": {
        "id": "q5tE4JDOWcOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MarioNet(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    c, h, w = input_dim\n",
        "\n",
        "    if h!= 84:\n",
        "      raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "    if w!= 84:\n",
        "      raise ValueError(f\"Expecting input width: 84, got: {h}\")\n",
        "    self.online = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = c, out_channels = 32, kernel_size = 8, stride = 4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4, stride = 2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3136, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, output_dim)\n",
        "    )\n",
        "    self.target = copy.deepcopy(self.online)\n",
        "    for p in self.target.parameters():\n",
        "      p.requires_grad = False\n",
        "\n",
        "  def forward(self, input, model):\n",
        "    if model == \"online\":\n",
        "      return self.online(input)\n",
        "    elif model == \"target\":\n",
        "      return self.target(input)"
      ],
      "metadata": {
        "id": "GAAmDQMpa_zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario:\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.gamma = 0.9\n",
        "    self.memory = deque(maxlen = 100000)\n",
        "    self.batch_size = 32\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.save_dir = save_dir\n",
        "    self.device = \"cpu\"\n",
        "    self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "    self.net = self.net.to(device = self.device)\n",
        "    self.exploration_rate = 1\n",
        "    self.exploration_rate_decay = 0.99999975\n",
        "    self.exploration_rate_min = 0.1\n",
        "    self.curr_step = 0\n",
        "    self.save_every = 5e5\n",
        "    self.optimizer = torch.optim.Adam(self.net.parameters(), lr = 0.00025)\n",
        "    self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "    self.burnin = 1e4\n",
        "    self.learn_every = 3\n",
        "    self.sync_every = 1e4\n",
        "\n",
        "  def act(self, state):\n",
        "    if np.random.rand() < self.exploration_rate:\n",
        "      action_idx = np.random.randint(self.action_dim)\n",
        "    else:\n",
        "      state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "      state = torch.tensor(state, device = self.device).unsqueeze(0)\n",
        "      action_values = self.net(state, model = \"online\")\n",
        "      action_idx = torch.argmax(action_values, axis = 1).item()\n",
        "    self.exploration_rate *= self.exploration_rate_decay\n",
        "    self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "    self.curr_step += 1\n",
        "    return action_idx\n",
        "\n",
        "  def cache(self, experience):\n",
        "    def first_if_tuple(x):\n",
        "      return x[0] if isinstance(x, tuple) else x\n",
        "    state = first_if_tuple(state).__array__()\n",
        "    next_state = first_if_tuple(next_state).__array__()\n",
        "    state = torch.tensor(state, device = self.device)\n",
        "    next_state = torch.tensor(next_state, device = self.device)\n",
        "    action = torch.tensor([action], device = self.device)\n",
        "    reward = torch.tensor([done], device = self.device)\n",
        "    self.memory.append((state, next_state, action, reward, done,))\n",
        "\n",
        "  def recall(self):\n",
        "    batch = random.sample(self.memory, self.batch_size)\n",
        "    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "\n",
        "  def td_estimate(self, state, action):\n",
        "    current_Q = self.net(state, model = \"online\") [\n",
        "        np.arange(0, self.batch_size), action\n",
        "    ]\n",
        "    return current_Q\n",
        "  @torch.no_grad()\n",
        "\n",
        "  def td_target(self, reward, next_state, done):\n",
        "    next_state_Q = self.net(next_state, model = \"online\")\n",
        "    best_action = torch.argmax(next_state_Q, axis = 1)\n",
        "    next_Q = self.net(next_state, model = \"target\") [\n",
        "        np.arange(0, self.batch_size), best_action\n",
        "    ]\n",
        "    return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
        "\n",
        "  def update_Q_online(self, td_estimate, td_target):\n",
        "    loss = self.loss_fn(td_estimate, td_target)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "  def sync_Q_target(self):\n",
        "    self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "\n",
        "  def save(self):\n",
        "    save_path = (self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chpkt\")\n",
        "    torch.save(dict(model = self.net.state_dict(), exploration_rate = self.exploration_rate), save_path)\n",
        "    print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
        "\n",
        "  def learn(self):\n",
        "    if self.curr_step % self.sync_every == 0:\n",
        "      self.sync_Q_target()\n",
        "    if self.curr_step % self.save_every == 0:\n",
        "      self.save()\n",
        "    if self.curr_step < self.burnin:\n",
        "      return None, None\n",
        "    \n",
        "    state, next_state, action, reward, done = self.recall()\n",
        "    td_est = self.td_estimate(state, action)\n",
        "    td_tgt = self.td_target(reward, next_state, done)\n",
        "    loss = self.update_Q_online(td_est, td_tgt)\n",
        "    return (td_est.mean().item(), loss)"
      ],
      "metadata": {
        "id": "sxSTKg9LW1VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ],
      "metadata": {
        "id": "24KDvep2sM8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 10\n",
        "for e in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # Remember\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # Logging\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 20 == 0:\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
      ],
      "metadata": {
        "id": "zh6WKdFGNjip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IUjCRUglNnDf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}